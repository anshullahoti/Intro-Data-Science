{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Homework 3: (adapted from CS194-16) Introduction to Data Science csc599.70\n",
    " \n",
    " \n",
    "\n",
    "**Name**: Brandon Chin\n",
    "\n",
    "**Student ID**: 16142938\n",
    "\n",
    "\n",
    "Homework 3: Introduction to Machine Learning: Clustering and Regression\n",
    "===\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, we will use machine learning techniques to perform data analysis and learn models about our data. We will use a real world music dataset from [Last.fm](http://last.fm) for this assignment. There are two parts to this assignment: In the first part we will look at Unsupervised Learning with clustering and in the second part, we will study Supervised Learning. The play data (and user/artist matrix) comes from the [Last.fm 1K Users dataset](http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html), while the tags come from [the Last.fm Music Tags dataset](http://musicmachinery.com/2010/11/10/lastfm-artisttags2007/). You won't have to interact with these datasets directly, because we've already preprocessed them for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a branch of artifical intelligence where we try to find hidden structure within data. \n",
    "For example, lets say you are hired as a data scientist at a cool new music playing startup. You are given access to \n",
    "logs from the product and are asked find out what kinds of music are played on your website and how you can promote songs that will be \n",
    "popular. In this case we wish to extract some structure from the raw data we have using machine learning.\n",
    "\n",
    "There are two main kinds of machine learning algorithms:\n",
    "\n",
    "1. Unsupervised Learning - is the branch where we don't have any ground truth (or labeled data) that can help our training process. There are many approaches to unsupervised learning which includes topics like Clustering,  Mixture Models, Hidden Markov Models etc. In this assignment we will predominantly look at clustering.\n",
    "2. Supervised Learning - we have training data which is labeled (either manually or from historical data) and we try to make predictions about those labels on new, unlabeled, data. There are similarly several approaches to supervised learning - various classification and regression techniques all the way up to Support Vector Machines and Convolutional Neural Networks. In this assignment we'll explore two regression algorithms - Least Squares Linear Regression and Regression Trees. \n",
    "\n",
    "Many of the techniques you'll be using (like testing on a validation set) are of critical importance to the modeling process, regardless of the technique you're using, so keep these in mind in your future modeling efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "Your assignment is to use machine learning algorithms for two tasks on a real world music dataset from Last.fm. The goal in the first part is to cluster artists and try to discover all artists that belong a certain genre. In the second part, we'll use the same dataset and attempt to predict how popular a song will be based on a number of features of that song. One component will involve incorporating cluster information into the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "\n",
    "Data files for this assignment can be found via this link. Either download directly to your VM, or to your host and use drag-and-drop. Create a directory ~/HWs/HW3 and put the archive file there. Unpack it with <code>tar xvzf file</code>\n",
    "\n",
    "The zip file includes the following files:\n",
    "\n",
    "* **artists-tags.txt**, User-defined tags for top artists\n",
    "* **userart-mat-training.csv**, Training data containing a matrix mapping artist-id to users who have played songs by the artists\n",
    "* **userart-mat-test.csv**, Test data containing a matrix mapping artist-id to users who have played songs by the artists\n",
    "* **train_model_data.csv**, Aggregate statsitstics and features about songs we'll use to train regression models.\n",
    "* **validation_model_data.csv**, Similar statistics computed on a hold-out set of users and songs that we'll use to validate our regression models.\n",
    "\n",
    "We will explain the datasets and how they need to used in the assignment sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "\n",
    "Complete the all the exercises below and turn in a write up in the form of an IPython notebook, that is, **an .ipynb file**.\n",
    "The write up should include your code, answers to exercise questions, and plots of results.\n",
    "This time you will submit it directly to bCourses. \n",
    "\n",
    "We recommend that you do your work in a **copy of this notebook**, in case there are changes that need to be made that are pushed out via github. In this notebook, we provide code templates for many of the exercises. They are intended to help with code re-use, since the exercises build on each other, and are highly recommended. Don't forget to include answers to questions that ask for natural language responses, i.e., in English, not code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "#### Code\n",
    "\n",
    "This assignment can be done with basic python, matplotlib and scikit-learn.\n",
    "Feel free to use Pandas, too, which you may find well suited to several exercises.\n",
    "As for other libraries, please check with course staff whether they're allowed.\n",
    "\n",
    "You're not required to do your coding in IPython, so feel free to use your favorite editor or IDE.\n",
    "But when you're done, remember to put your code into a notebook for your write up.\n",
    "\n",
    "#### Collaboration\n",
    "\n",
    "This assignment is to be done individually.  Everyone should be getting a hands on experience in this course.  You are free to discuss course material with fellow students, and we encourage you to use Internet resources to aid your understanding, but the work you turn in, including all code and answers, must be your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Preliminaries\n",
    "\n",
    "### Exercise 0\n",
    "\n",
    "Read in the file **artists-tags.txt** and store the contents in a DataFrame. The file format for this file is `ArtistID|ArtistName|Tag|Count`. The fields mean the following:\n",
    "\n",
    "1. ArtistID : a unique id for an artist (Formatted as a [MusicBrainz Identifier](https://musicbrainz.org/doc/MusicBrainz_Identifier))\n",
    "2. ArtistName: name of the artist\n",
    "3. Tag: user-defined tag for the artist\n",
    "4. Count: number of times the tag was applied\n",
    "\n",
    "Similarly, read in the file **userart-mat-training.csv** . The file format for this file is `ArtistID, user_000001, user_000002, .... user_001000`. i.e. There are 846 such columns in this file and each column has a value 1 if the particular user played a song from this artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags 100782\n",
      "Number of artists 20863\n"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"/home/brandon/Documents/Data Science/Unit3-Regression-Classification-Clustering/Homework3\"\n",
    "\n",
    "def parse_artists_tags(filename):\n",
    "    df = pd.read_csv(filename, sep=\"|\", names=[\"ArtistID\", \"ArtistName\", \"Tag\", \"Count\"])\n",
    "    return df\n",
    "\n",
    "def parse_user_artists_matrix(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    return df\n",
    "\n",
    "artists_tags = parse_artists_tags(DATA_PATH + \"/artists-tags.txt\")\n",
    "user_art_mat = parse_user_artists_matrix(DATA_PATH + \"/userart-mat-training.csv\")\n",
    "\n",
    "print (\"Number of tags %d\" % len(artists_tags['Tag'].unique()))\n",
    "print (\"Number of artists %d\" % len(artists_tags['ArtistName'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArtistID</th>\n",
       "      <th>ArtistName</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000077f7-26b1-4710-80cc-f6beddbdd157</td>\n",
       "      <td>Ryan Adams and The Cardinals</td>\n",
       "      <td>I love you baby can I have some more</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000077f7-26b1-4710-80cc-f6beddbdd157</td>\n",
       "      <td>Ryan Adams and The Cardinals</td>\n",
       "      <td>alt country</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000077f7-26b1-4710-80cc-f6beddbdd157</td>\n",
       "      <td>Ryan Adams and The Cardinals</td>\n",
       "      <td>whoa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00034ede-a1f1-4219-be39-02f36853373e</td>\n",
       "      <td>O Rappa</td>\n",
       "      <td>Artist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00034ede-a1f1-4219-be39-02f36853373e</td>\n",
       "      <td>O Rappa</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ArtistID                    ArtistName  \\\n",
       "0  000077f7-26b1-4710-80cc-f6beddbdd157  Ryan Adams and The Cardinals   \n",
       "1  000077f7-26b1-4710-80cc-f6beddbdd157  Ryan Adams and The Cardinals   \n",
       "2  000077f7-26b1-4710-80cc-f6beddbdd157  Ryan Adams and The Cardinals   \n",
       "3  00034ede-a1f1-4219-be39-02f36853373e                       O Rappa   \n",
       "4  00034ede-a1f1-4219-be39-02f36853373e                       O Rappa   \n",
       "\n",
       "                                    Tag  Count  \n",
       "0  I love you baby can I have some more      1  \n",
       "1                           alt country      2  \n",
       "2                                  whoa      1  \n",
       "3                                Artist      1  \n",
       "4                                 Black      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artists_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Finding genres by clustering\n",
    "\n",
    "The first task we will look at is how to discover artist genres by only looking at data from plays on Last.fm. One of the ways to do this is to use clustering. To evaluate how well our clustering algorithm performs we will use the user-generated tags and compare those to our clustering results. \n",
    "\n",
    "### 1.1 Data pre-processing\n",
    "\n",
    "Last.fm allows users to associate tags with every artist (See the [top tags](http://www.last.fm/charts/toptags) for a live example). However as there are a number of tags associated with every artists, in the first step we will pre-process the data and get the most popular tag for an artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "**a**. For every artist in **artists_tags** calculate the most frequently used tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArtistID</th>\n",
       "      <th>ArtistName</th>\n",
       "      <th>TopTag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000077f7-26b1-4710-80cc-f6beddbdd157</td>\n",
       "      <td>Ryan Adams and The Cardinals</td>\n",
       "      <td>alt country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>00034ede-a1f1-4219-be39-02f36853373e</td>\n",
       "      <td>O Rappa</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>00050add-f633-4901-8d93-6f88c640c0da</td>\n",
       "      <td>9 Inch Dix</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>000b1990-4dd8-4835-abcd-bb6038c13ac7</td>\n",
       "      <td>Hayden</td>\n",
       "      <td>indie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>000ba849-700e-452e-8858-0db591587e4a</td>\n",
       "      <td>The Mutton Birds</td>\n",
       "      <td>New Zealand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ArtistID                    ArtistName  \\\n",
       "1    000077f7-26b1-4710-80cc-f6beddbdd157  Ryan Adams and The Cardinals   \n",
       "54   00034ede-a1f1-4219-be39-02f36853373e                       O Rappa   \n",
       "59   00050add-f633-4901-8d93-6f88c640c0da                    9 Inch Dix   \n",
       "125  000b1990-4dd8-4835-abcd-bb6038c13ac7                        Hayden   \n",
       "163  000ba849-700e-452e-8858-0db591587e4a              The Mutton Birds   \n",
       "\n",
       "          TopTag  \n",
       "1    alt country  \n",
       "54          rock  \n",
       "59           rap  \n",
       "125        indie  \n",
       "163  New Zealand  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a data structure that contains (artist id, artist name, top tag) for every artist\n",
    "import numpy as np\n",
    "\n",
    "def calculate_top_tag(all_tags):\n",
    "    # 1. Sort rows by ArtistID, Count in ascending order\n",
    "    # 2. Drop duplicate values of each ArtistID except the last row (which contains the highest Count)\n",
    "    # 3. Drop the Count column from the data frame (axis=1 refers to the column)\n",
    "    return all_tags.sort_values(['ArtistID', 'Count']).drop_duplicates(subset=['ArtistID'], keep='last').drop('Count', axis=1)\n",
    "\n",
    "top_tags = calculate_top_tag(artists_tags)\n",
    "top_tags.columns = ['ArtistID', 'ArtistName', 'TopTag']    # Rename column headers\n",
    "top_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top tag for Nirvana is Grunge\n"
     ]
    }
   ],
   "source": [
    "# Print the top tag for Nirvana\n",
    "# Artist ID for Nirvana is 5b11f4ce-a62d-471e-81fc-a69a8278c7da\n",
    "# Should be 'Grunge'\n",
    "\n",
    "def get_tag(top_tags, artist_id):\n",
    "    row = top_tags.loc[top_tags['ArtistID'] == artist_id]\n",
    "    return row.iloc[0, 2]\n",
    "\n",
    "print (\"Top tag for Nirvana is %s\" % get_tag(top_tags, '5b11f4ce-a62d-471e-81fc-a69a8278c7da'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArtistID</th>\n",
       "      <th>user_000001</th>\n",
       "      <th>user_000002</th>\n",
       "      <th>user_000003</th>\n",
       "      <th>user_000004</th>\n",
       "      <th>user_000005</th>\n",
       "      <th>user_000006</th>\n",
       "      <th>user_000007</th>\n",
       "      <th>user_000008</th>\n",
       "      <th>user_000009</th>\n",
       "      <th>...</th>\n",
       "      <th>user_000890</th>\n",
       "      <th>user_000891</th>\n",
       "      <th>user_000892</th>\n",
       "      <th>user_000893</th>\n",
       "      <th>user_000894</th>\n",
       "      <th>user_000895</th>\n",
       "      <th>user_000896</th>\n",
       "      <th>user_000897</th>\n",
       "      <th>user_000898</th>\n",
       "      <th>user_000899</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c489cd1c-d8d3-4dcc-b7b9-dcf7cd95d2ed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ccbb935-33fd-4df8-ae2d-779497c5630a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c71b2cc1-2737-4f76-bbf2-9ee505b4d90d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0aa8294b-6332-4b65-b677-e3a1f8591d3b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67673557-2310-41d3-83b9-e6e0cb1d65d5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 847 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ArtistID  user_000001  user_000002  \\\n",
       "0  c489cd1c-d8d3-4dcc-b7b9-dcf7cd95d2ed            0            0   \n",
       "1  9ccbb935-33fd-4df8-ae2d-779497c5630a            0            0   \n",
       "2  c71b2cc1-2737-4f76-bbf2-9ee505b4d90d            0            0   \n",
       "3  0aa8294b-6332-4b65-b677-e3a1f8591d3b            0            0   \n",
       "4  67673557-2310-41d3-83b9-e6e0cb1d65d5            0            0   \n",
       "\n",
       "   user_000003  user_000004  user_000005  user_000006  user_000007  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   user_000008  user_000009     ...       user_000890  user_000891  \\\n",
       "0            0            0     ...                 0            0   \n",
       "1            0            0     ...                 0            0   \n",
       "2            0            0     ...                 0            0   \n",
       "3            0            0     ...                 0            0   \n",
       "4            0            0     ...                 0            0   \n",
       "\n",
       "   user_000892  user_000893  user_000894  user_000895  user_000896  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   user_000897  user_000898  user_000899  \n",
       "0            0            0            0  \n",
       "1            0            0            0  \n",
       "2            0            1            0  \n",
       "3            0            0            0  \n",
       "4            0            0            0  \n",
       "\n",
       "[5 rows x 847 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_art_mat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. To do clustering we will be using `numpy` matrices. Create a matrix from **user_art_mat** with every row in the matrix representing a single artist. The matrix will have 846 columns, one for whether each user listened to the artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17119, 847)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def create_user_matrix(input_data):\n",
    "    return input_data.values\n",
    "\n",
    "user_np_matrix = create_user_matrix(user_art_mat)\n",
    "\n",
    "print (user_np_matrix.shape)\n",
    "print(type(user_np_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 K-Means clustering\n",
    "\n",
    "Having pre-processed the data we can now perform clustering on the dataset. In this assignment we will be using the python library \n",
    "[scikit-learn](http://scikit-learn.org/stable/index.html) for our machine learning algorithms. scikit-learn provides an extensive\n",
    "library of machine learning algorithms that can be used for analysis. Here is a [nice flow chart](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) that shows various algorithms implemented\n",
    "and when to use any of them. In this part of the assignment we will look at K-Means clustering\n",
    "\n",
    "> **Note on terminology**: \"samples\" and \"features\" are two words you will come across frequently when you look at machine learning papers or documentation. \"samples\" refer to data points that are used as inputs to the machine learning algorithm. For example in our dataset each artist is a \"sample\". \"features\" refers to some representation we have for every sample. For example the list of 1s and 0s we have for each artist are \"features\". Similarly the bag-of-words approach from the previous homework produced \"features\" for each document.\n",
    "\n",
    "#### K-Means algorithm\n",
    "\n",
    "Clustering is the process of automatically grouping data points that are similar to each other. In the [K-Means algorithm](http://en.wikipedia.org/wiki/K-means_clustering) we start with `K` initially chosen cluster centers (or centroids). We then compute the distance of every point from the centroids and assign each point to the centroid. Next we update the centroids by averaging all the points in the cluster. Finally, we repeat the algorithm until the cluster centers are stable.\n",
    "\n",
    "### Running K-Means\n",
    "\n",
    "#### K-Means interface\n",
    "Take a minute to look at the scikit-learn interface for calling [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). The constructor of the KMeans class returns a `estimator` on which you can call [fit](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit) to perform clustering.\n",
    "\n",
    "#### K-Means parameters\n",
    "From the above description we can see that there are a few parameters which control the K-Means algorithm. We will look at one parameter specifically, the number of clusters used in the algorithm. The number of clusters needs to be chosen based on domain knowledge of the data. As we do not know how many genres exist we will try different values and compare the results.\n",
    "\n",
    "#### Timing your code\n",
    "We will also measure the performance of clustering algorithms in this section. You can time the code in a cell using the **%%time** [IPython magic](http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb) as the first line in the cell. \n",
    "\n",
    ">**Note**: By default, the scikit-learn KMeans implementation runs the algorithm 10 times with different center initializations. For this assignment you can run it just once by passing the `n_init` argument as 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "**a**. Run K-means using *5* cluster centers on the `user_np_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Run K-means using 5 cluster centers on user_np_matrix\n",
    "kmeans_5 = \n",
    "kmeans_5.fit(user_np_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Run K-means using *25* and *50* cluster centers on the `user_np_matrix`. Also measure the time taken for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans_25 = \n",
    "kmeans_25.fit(user_np_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans_50 = \n",
    "kmeans_50.fit(user_np_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Of the three algorithms, which setting took the longest to run ? Why do you think this is the case ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO - Answer question here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evaluating K-Means\n",
    "\n",
    "In addition to the speed comparisons we also wish to compare how good our clusters are. To do this we are first going to look at internal evaluation metrics. For internal evaluation we only use the input data and the clusters created and try to measure the quality of clusters. We will use a standard metric for this:\n",
    "\n",
    "#### Inertia\n",
    "Inertia is a metric that is used to estimate how close the data points in a cluster are. This is calculated as the sum of squared distance for each point to it's closest centroid, i.e., its assigned cluster center. The intution behind inertia is that clusters with lower inertia are better as it means closely related points form a cluster.Inertia is calculated by scikit-learn by default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a**. Print inertia for all the kmeans model computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Inertia for KMeans with 5 clusters = %lf \" % 0.0\n",
    "print \"Inertia for KMeans with 25 clusters =  %lf \" % 0.0\n",
    "print \"Inertia for KMeans with 50 clusters = %lf \" % 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Does KMeans run with 25 clusters have lower or greater inertia than the ones with 5 or 50 clusters ? Why do you think this is ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO: Answer question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Plot: \n",
    "A silhouette plot shows a lot of information about the quality of a clustering. You can read more about it <a href=\"http://en.wikipedia.org/wiki/Silhouette_%28clustering%29\">here</a>.\n",
    "\n",
    "Each sample x is assigned a score based on two distances:\n",
    "* a is the mean distance from x to other elements in its own cluster. \n",
    "* b is the mean distance from x to other elements in the next closest cluster to x.\n",
    "then the silhouette score is defined as $$\\frac{b-a}{\\max(a,b)}$$\n",
    "\n",
    "We want this number to be large, since it means the distances within the cluster are smaller than to the next best cluster. \n",
    "To construct the silhouette plot, we first group all the samples by the cluster containining them. \n",
    "Then within each group, we sort all the samples in that cluster by descending silhouette score.\n",
    "\n",
    "We start with the labels of the classified points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = kmeans_5.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the silhouette score for each sample. You dont need to do this for every sample, so define a variable n which is the number of samples to work with from now on. Use the sklearn \"silhouette_samples\" function to get the silhouette score for those n samples, and create a variable with the corresponding n labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "n=1000\n",
    "\n",
    "slabels = # subset of n labels\n",
    "sscore = # subset of n silhouette scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to organize the scores into groups by the label. Create a DataFrame with two columns \"ClusterId\" and \"Silhouette Score\", and fill it with the n labels and n scores from above. Sort the frame first by ClusterId, then by Silhouette Score, so that cluster samples are grouped together. Now extract the sorted Silhouette Score column as a matrix. Plot it using pylab's \"barh\" routine, which plots horizontal bars. You should also print out a dataframe with counts of samples for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pylab import *\n",
    "\n",
    "df = Dataframe(...)\n",
    "\n",
    "dfs = df.sort(...)\n",
    "sscore = dfs[\"Silhouette Score\"].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c** What do you notice about the sizes of the clusters? What about the average silhouette score per cluster? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO: Answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 External Evaluation\n",
    "While internal evaluation is useful, a better method for measuring clustering quality is to do external evaluation. This might not be possible always as we may not have ground truth data available. In our application we will use `top_tags` from before as our ground truth data for external evaluation. We will first compute purity and accuracy and finally we will predict tags for our **test** dataset.\n",
    "\n",
    "#### Exercise 4\n",
    "\n",
    "**a**. As a first step we will need to **join** the `artist_tags` data with the set of labels generated by K-Means model. That is, for every artist we will now have the top tag, cluster id and artist name in a data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return a data structure that contains artist_id, artist_name, top tag, cluster_label for every artist\n",
    "def join_tags_labels(artists_data, user_data, kmeans_model):\n",
    "    pass\n",
    "\n",
    "# Run the function for all the models\n",
    "kmeans_5_joined = None\n",
    "kmeans_25_joined = None\n",
    "kmeans_50_joined = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Next we need to generate a genre for every cluster id we have (the cluster ids are from 0 to N-1). You can do this by **grouping** the data from the previous exercise on cluster id. \n",
    "\n",
    "One thing you might notice is that we typically get a bunch of different tags associated with every cluster. How do we pick one genre or tag from this ? To cover various tags that are part of the cluster, we will pick the **top 5** tags in each cluster and save the list of top-5 tags as the genre for the cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return a data structure that contains cluster_id, list of top 5 tags for every cluster\n",
    "def assign_cluster_tags(joined_data):\n",
    "    pass\n",
    "    \n",
    "kmeans_5_genres = None\n",
    "kmeans_25_genres = None\n",
    "kmeans_50_genres = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Purity\n",
    "**Purity** measures the frequency of data belonging to the same cluster sharing the same class label i.e. if we have a number of items in a cluster how many of those items have the same label ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Compute the purity for each of our K-Means models. To do this find the top tags of all artists that belong to a cluster. Check what fraction of these tags are covered by the top 5 tags of the cluster. Average this value across all clusters. **HINT**: We used similar ideas to get the top 5 tags in a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cluster_purity(joined_data):\n",
    "    pass\n",
    "    \n",
    "print \"Purity for KMeans with 5 centers %lf \" % 0.0\n",
    "print \"Purity for KMeans with 25 centers %lf \" % 0.0\n",
    "print \"Purity for KMeans with 50 centers %lf \" % 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** What do the numbers tell you about the models? Do you have a favorite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Evaluating Test Data\n",
    "Finally we can treat the clustering model as a multi-class classifier and make predictions on external test data. To do this we load the test data file **userart-mat-test.csv** and for every artist in the file we use the K-Means model to predict a cluster. We mark our prediction as successful if the artist's top tag belongs to one of the five tags for the cluster. \n",
    "\n",
    "#### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a** Load the testdata file and create a NumPy matrix named user_np_matrix_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_art_mat_test = parse_user_artists_matrix(DATA_PATH + \"/userart-mat-test.csv\")\n",
    "# NOTE: the astype(float) converts integer to floats here\n",
    "user_np_matrix_test = create_user_matrix(user_art_mat_test).astype(float)\n",
    "\n",
    "user_np_matrix_test.shape # Should be (1902, 846)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** For each artist in the test set, call **[predict](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict)** to get the predicted cluster. Join the predicted labels with test artist ids. Return 'artist_id', 'predicted_label' for every artist in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For every artist return a list of labels\n",
    "def predict_cluster(test_data, test_np_matrix, kmeans_model):\n",
    "    pass\n",
    "\n",
    "# Call the function for every model from before\n",
    "kmeans_5_predicted = None\n",
    "kmeans_25_predicted = None\n",
    "kmeans_50_predicted = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Get the tags for the predicted genre and the tag for the artist from `top_tags`. Output the percentage of artists for whom the top tag is one of the five that describe its cluster. This is the *recall* of our model.\n",
    ">NOTE: Since the tag data is not from the same source as user plays, there are artists in the test set for whom we do not have top tags. You should exclude these artists while making predictions and while computing the recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate recall for our predictions\n",
    "def verify_predictions(predicted_artist_labels, cluster_genres, top_tag_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d**. Print the recall for each KMeans model. We define recall as num_correct_predictions / num_artists_in_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use verify_predictions for every model\n",
    "print \"Recall of KMeans with 5 centers %lf \" % 0.0\n",
    "print \"Recall of KMeans with 25 centers %lf \" % 0.0\n",
    "print \"Recall of KMeans with 50 centers %lf \" % 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Regression Models - Predicting Song Popularity\n",
    "\n",
    "In this section of the assignment you'll be building a model to predict the number of plays a song will get. Again, we're going to be using scikit-learn to train and evaluate regression models, and pandas to pre-process the data.\n",
    "\n",
    "In the process, you'll encounter some modeling challenges and we'll look at how to deal with them.\n",
    "\n",
    "We've started with the same data as above, but this time we've pre-computed a number of song statistics for you.\n",
    "\n",
    "These are:\n",
    "\n",
    "1. plays - the number of times a song has been played.\n",
    "1. pctmale - percentage of the plays that came from users who self-identified as \"male\".\n",
    "1. age - average age of the listener.\n",
    "1. country1 - the country of the users that listened to this song most.\n",
    "1. country2 - the country of the users that listened to this song second most.\n",
    "1. country3 - the country of the users that listened to this song third most.\n",
    "1. pctgt1 - Percentage of plays that come from a user who's played the song more than once.\n",
    "1. pctgt2 - Percentage of plays that come from a user who's played the song more than twice.\n",
    "1. pctgt5 - Percentage of plays that come from a user who's played the song more than five times.\n",
    "1. cluster - The \"cluster number\" of the artist associated with this song - similar to what you came up with above. We chose 25 clusters fairly arbitrarily.\n",
    "\n",
    "### 2.1 Data Exploration\n",
    "#### Exercise 6\n",
    "\n",
    "**a**. Let's start by loading up the data - we've provided a \"training set\" and a \"validation set\" for you to test your models on. The training set are the examples that we use to create our models, while the validation set is a dataset we \"hold out\" from the model fitting process, we use these examples to test whether our models accurately predict new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + \"/train_model_data.csv\")\n",
    "validation = pd.read_csv(DATA_PATH + \"/validation_model_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've got the data loaded, play around with it, generate some descriptive statistics, and get a feel for what's in the data set. For the categorical variables try pandas \".count_values()\" on them to get a sense of the most likely distributions (countries, etc.). \n",
    "\n",
    "**b**. In the next cell put some commands you ran to get a feel for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Your commands for data exploration here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Next, create a pairwise scatter plot of the columns: plays, pctmale, age, pctgt1, pctgt2, pctgt5. (_Hint: we did this in lab!_)\n",
    "\n",
    "Do you notice anything about the data in this view? What about the relationship between plays and other columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Your commands to generate a scatter plot here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO: What do you notice about the data in this view? Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Prep and Intro to Linear Regression\n",
    "\n",
    "*scikit-learn* does a number of things very well, but one of the things it doesn't handle easily is categorical or missing data. Categorical data is data that can take on a finite set of values, e.g. a categorical variable might be the color of a stop light (Red, Yellow, Green), this is in contrast with continuous variables like real numbers in the range -Infinity to +Infinity. There is another common type of data called \"ordinal\" that can be thought of as categorical data that has a natural ordering, like: Cold, Warm, Hot. We won't be dealing with this kind of data here, but having that kind of ranking opens up the use of certain other statistical methods.\n",
    "\n",
    "\n",
    "#### Exercise 7\n",
    "\n",
    "**a**. For the first part of the exercise, let's eliminate categorical variables, and *impute* missing values with pandas. Write a function to drop all categorical variables from the data set, and return two pandas data frames:\n",
    "\n",
    "1. A data frame with all categorical items and a user-specified response column removed.\n",
    "2. A data frame that contains only the response column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def basic_prep(data, col):\n",
    "    #TODO - make a copy of the original dataset but with the categorical variables removed! *Cluster* should be thought of as a \n",
    "    #categorical variable and should be removed! Make use of pandas \".drop\" function.\n",
    "    \n",
    "    #TODO - impute missing values with the mean of those columns, use pandas \".fillna\" function to accomplish this.\n",
    "    \n",
    "    pass\n",
    "\n",
    "#This will create two new data frames, one that contains training data - in this case all the numeric columns,\n",
    "#and one that contains response data - in this case, the \"plays\" column.\n",
    "train_basic_features, train_basic_response = basic_prep(train, 'plays')\n",
    "validation_basic_features, validation_basic_response = basic_prep(validation, 'plays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to train a linear regression model. This is likely the most widely used model for fitting data out there today - you've probably seen it before, maybe even used it in Excel. The goal of linear modeling, is to fit a **linear equation** that maps a set of **input features** to a numerical **response**. This equation is called a **model**, and can be used to make predictions about the response of similar input features. For example, imagine we have a dataset of electricity prices ($p$) and outdoor temperature ($t$), and we want to predict, given temperature, what electricity price will be. A simple way to model this is with an equation that looks something like $p = basePrice + factor*t$. When we **fit** a model, we are estimating the parameters ($basePrice$ and $factor$) that best fit our data. This is a very simple linear model, but you can easily imagine extending this to situations where you need to estimate several parameters.\n",
    "\n",
    ">**Note**: It is possible to fill a semester with linear models (and classes in other departments do!), and there are innumerable issues to be aware of when you fit linear models, so this is just the tip of the iceberg - don't dismiss linear models outright based on your experiences here!\n",
    "\n",
    "A linear model models the data as a **linear combination** of the model and its weights. Typically, the model is written with something like the following form: $y = X\\theta + \\epsilon$, and when we fit the model, we are trying to find the value of $\\theta$ that minimizes the **loss** of the model. In the case of regression models, the loss is often represented as $\\sum (y - X\\theta)^2$ - or the squared distance between the prediction and the actual value.\n",
    "\n",
    "In the code below, `X` refers to the the training features, `y` refers to the training response, `Xv` refers to the validation features and yv refers to the validation response. Note that `X` is a matrix (or a `DataFrame`) with the shape $n \\times d$ where $n$ is the number of examples and $d$ is the number of features in each example, while `y` is a vector of length $n$ (one response per example).\n",
    "\n",
    "Our goal with this assignment is to accurately estimate the number of plays a song will get based on the features we know about it.\n",
    "\n",
    "The score we'll be judging the models on is called $R^2$, which is a measure of how well the model fits the data. It can be thought of roughly as the percentage of the variance that the model explains. \n",
    "\n",
    "#### Exercise 9\n",
    "\n",
    "**a.** Fit a `LinearRegression` model with scikit-learn and return the model score on both the training data and the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def fit_model(X, y):\n",
    "    #TODO - Write a function that fits a linear model to a dataset given a column of values to predict.\n",
    "    pass\n",
    "\n",
    "def score_model(model, X, y, Xv, yv):\n",
    "    #TODO - Write a function that returns scores of a model given its training \n",
    "    #features and response and validation features and response. \n",
    "    #The output should be a tuple of two model scores.\n",
    "    pass\n",
    "\n",
    "def fit_model_and_score(data, response, validation, val_response):\n",
    "    #TODO - Given a training dataset, a validation dataset, and the name of a column to predict, \n",
    "    #Using the model's \".score()\" method, return the model score on the training data *and* the validation data\n",
    "    #as a tuple of two doubles.\n",
    "    pass\n",
    "    #END TODO\n",
    "\n",
    "print fit_model_and_score(train_basic_features, train_basic_response, validation_basic_features, validation_basic_response)\n",
    "\n",
    "model = fit_model(train_basic_features, train_basic_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize that this may be your first experience with linear models - but that's a pretty low $R^2$ - we're looking for scores significantly higher than 0, and the maximum is a 1. \n",
    "\n",
    "So what happened? Well, we've modeled a **linear** response to our input features, but the variable we're modeling (plays) clearly has a non-linear relationship with respect to the input features. It roughly follows a **power-law** distribution, and so modeling it in linear space yields a model with estimates that are way off.\n",
    "\n",
    "We can verify this by looking at a plot of the model's residuals - that is, the difference between the training responses and the predictions. A good model would have residuals with two properties:\n",
    "\n",
    "1. Small in absolute value. \n",
    "1. Evenly distributed about the true values.\n",
    "\n",
    "**b.** Write a function to calculate the residuals of the model, and plot those with a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def residuals(features, y, model):\n",
    "    #TODO - Write a function that calculates model residuals given input features, ground truth, and the model.\n",
    "    pass\n",
    "\n",
    "#TODO - Plot the histogram of the residuals of your current model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the structure in the plot? This means we've got more modeling to do before we can call it a day! It satisfies neither of our properties - we're often way wrong with our predictions, and seem to systematically **under** predict the number of plays a song will get.\n",
    "\n",
    "What happens if we try and predict the $log$ of number of plays? This controls the exponential behaviour of plays, and gives less weight to the case where our prediction was off by 100 when the true answer was 1000. \n",
    "\n",
    "#### Exercise 8\n",
    "**a.** Adapt your model fitting from above to fit the **log** of the nubmer of plays as your response variable. Print the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#TODO - Using what you built above, build a model using the log of the number of plays as the response variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** You should see a significantly better $R^2$ and validation $R^2$, though still pretty low. Take a look at the model residuals again, do they look any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO Plot residuals of your log model. Note - we want to see these on a \"plays\" scale, not a \"log(plays)\" scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There must be something we can do here to build a better model. Let's try incorporating country and cluster information.\n",
    "\n",
    "### 2.3 Linear Modeling with Categorical Variables: One-Hot Encoding\n",
    "\n",
    "Linear models expect **numbers** for input features. But we have some features that we think could be useful that are **discrete** or **categorical**. How do we represent these as numbers?\n",
    "\n",
    "One solution is something called one-hot encoding. Basically, we map a discrete space to a vector of binary indicators, then use these indicators as numbers.\n",
    "\n",
    "For example, if I had an input column that could take on the values {$RED$, $GREEN$, $BLUE$}, and I wanted to model this with one-hot-encoding, I could use a map:\n",
    "\n",
    "* $RED = 001$\n",
    "* $GREEN = 010$\n",
    "* $BLUE = 100$\n",
    "\n",
    "We use this representation instead of traditional binary numbers to keep these features independent of one another.\n",
    "\n",
    "Once we've established this representation, we replace the columns in our dataset with their one-hot-encoded values. Then, we can fit a linear model on the data once it's encoded this way!\n",
    "\n",
    "Statisticians and econometricians call these types of binary variables *dummy variables*, but we're going to call it one-hot encoding, because that sounds cooler.\n",
    "\n",
    "Scikit-learn has functionality to transform values to this encoding built in, so we'll leverage that. The functionality is called `DictVectorizer` in scikit-learn. The idea is that you feed a `DictVectorizer` a bunch of examples of your data (that's the `vec.fit` line), and it builds a map from a categorical variable to a one-hot encoded vector like we have in the color example above. Then, you can use this object to translate from categorical values to sequences of numeric ones as we do with `vec.transform`. In the example below, we fit a vectorizer on the training data and use the same vectorizer on the validation data so that the mapping is consistent and we don't run into issues if the categories don't match perfectly between the two data sets.\n",
    "\n",
    "#### Exercise 9\n",
    "**a.** Use the code below to generate new training and validation datasets for the datasets *with* the categorical features in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction\n",
    "\n",
    "def one_hot_dataframe(data, cols, vec=None):\n",
    "    \"\"\" Takes a dataframe and a list of columns that need to be encoded.\n",
    "        Returns a tuple comprising the data, and the fitted vectorizor.\n",
    "        \n",
    "        Based on https://gist.github.com/kljensen/5452382\n",
    "    \"\"\"\n",
    "    if vec is None:\n",
    "        vec = feature_extraction.DictVectorizer()\n",
    "        vec.fit(data[cols].to_dict(outtype='records'))\n",
    "    \n",
    "    vecData = pd.DataFrame(vec.transform(data[cols].to_dict(outtype='records')).toarray())\n",
    "    vecData.columns = vec.get_feature_names()\n",
    "    vecData.index = data.index\n",
    "    \n",
    "    data = data.drop(cols, axis=1)\n",
    "    data = data.join(vecData)\n",
    "    return (data, vec)\n",
    "\n",
    "def prep_dset(data, col, vec=None):\n",
    "    #Convert the clusters to strings.\n",
    "    new_data = data\n",
    "    new_data['cluster'] = new_data['cluster'].apply(str)\n",
    "    \n",
    "    #Encode the data with OneHot Encoding.\n",
    "    new_data, vec = one_hot_dataframe(new_data, ['country1','cluster'], vec)\n",
    "\n",
    "    #Eliminate features we don't want to use in the model.\n",
    "    badcols = ['country2','country3','artid','key','age']\n",
    "    new_data = new_data.drop(badcols, axis=1)\n",
    "    \n",
    "    new_data = new_data.fillna(new_data.mean())\n",
    "    \n",
    "    return (new_data.drop([col], axis=1), pd.DataFrame(new_data[col]), vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cats_features, train_cats_response, vec = prep_dset(train, 'plays')\n",
    "validation_cats_features, validation_cats_response, _ = prep_dset(validation, 'plays', vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Now that you've added the categorical data, let's see how it works with a linear model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fit_model_and_score(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a much better $R^2$ for the training data, but a much *worse* one for the validation data. What happened?\n",
    "\n",
    "This is a phenomenon called **overfitting** - our model has too many degrees of freedom (one parameter for each of the 100+ features of this dataset. This means that while our model fits the training data reasonably well, but at the expense of being too specific to that data.\n",
    "\n",
    "John Von Neumann famously said [\"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk!\"](http://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/).\n",
    "\n",
    "### 2.4 Non-linear modeling and Regression Trees\n",
    "So, we're at an impasse. We didn't have enough features and our model performed poorly, we added too many features and our model looked good on training data, but not so good on test data.\n",
    "\n",
    "What's a modeler to do? \n",
    "\n",
    "There are a couple of ways of dealing with this situation - one of them is called **regularization**, which you might try on your own (see `RidgeRegression` or `LassoRegression` in scikit-learn), another is to use a model which captures **non-linear** relationships between the features and the response variable.\n",
    "\n",
    "One such type of model was pioneered here at Berkeley, by the late, great Leo Breiman. These models are called **regression trees**.\n",
    "\n",
    "The basic idea behind regression trees is to recursively partition the dataset into subsets that are *similar with respect to the response variable*. \n",
    "\n",
    "If we take our temperature example, we might observe a non-linear relationship - electricity gets expensive when it's cold outside because we use the heater, but it also gets expensive when it's too hot outside because we run the air conditioning. \n",
    "\n",
    "A decision tree model might dynamically elect to split the data on the temperature feature, and estimate high prices both for hot and cold, with lower prices for more Berkeley-like temperatures. Go read the [scikit-learn decision trees documentation](http://scikit-learn.org/stable/modules/tree.html) for more background.\n",
    "\n",
    "#### Exercise 10\n",
    "**a.** Using the scikit learn `DecsionTreeRegressor` API, write a function that fits trees with the parameter 'max_depth' exposed to the user, and set to 10 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "def fit_tree(X, y, depth=10):\n",
    "    ##TODO: Using the DecisionTreeRegressor, train a model to depth 10.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** You should be able to use your same scoring function as above to compute your model scores. Write a function that fits a tree model to your training set and returns the model's score for both the training set and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_model_and_score_tree(train_features, train_response, val_features, val_response):\n",
    "    ##TODO: Fit a tree model and report the score on both the training set and test set.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Report the scores on the training and test data for both the basic features and the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print fit_model_and_score_tree(train_basic_features, train_basic_response, validation_basic_features, validation_basic_response)\n",
    "print fit_model_and_score_tree(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray - we've got a model that performs well on the training data set *and* the validation dataset. Which one is better? Why do you think that is. Try varying the depth of the decision tree (from, say, 2 to 20) and see how either data set does with respect to training and validation error. \n",
    "\n",
    "**d.** Now, let's build a tree to depth 3 and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import StringIO \n",
    "import pydot \n",
    "from IPython.display import Image\n",
    "\n",
    "tmodel = fit_tree(train_basic_features, train_basic_response, 3)\n",
    "\n",
    "def display_tree(tmodel):\n",
    "    dot_data = StringIO.StringIO() \n",
    "    tree.export_graphviz(tmodel, out_file=dot_data) \n",
    "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "    return Image(graph.create_png())\n",
    "    \n",
    "display_tree(tmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.** What is the tree doing? It looks like it's making a decision on variables X[4] and X[2] - can you briefly describe, in words, what the tree is doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO: Your answer goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f.** Finally, let's take a look at variable importance for a tree trained to 10 levels - this is a more formal way of deciding which features are important to the tree. The metric that scikit-learn calculates for feature importance is called GINI importance, and measures how much total 'impurity' is removed by splits from a given node. Variables that are highly discriminitive (e.g. ones that occur frequently throughout the tree) have higher GINI scores. You can read more about these scores [here](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmodel = fit_tree(train_basic_features, train_basic_response, 10)\n",
    "\n",
    "pd.DataFrame(tmodel.feature_importances_, train_basic_features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g.** What do you notice? Is the output interpretable? How would you explain this to someone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TODO: Your answer goes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
